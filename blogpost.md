# Introduction
*(Do we part from FROMAGe ("our paper") or from visual arithmetics ("our contribution")?)*\
Large language models (LLMs) have been the driving force behind recent advances in Artificial Intelligence (AI). Different approaches have been proposed on how to leverage their capabilities for multimodal input and output. Training a linear layer between two models of different modalities has been shown to be an efficient way of enabling them to overcome the modality gap.\
Arithmetic operations between word embeddings have been able to discover semantic similarities. However, previous approaches had textual output as the result of multimodal arithmetics. We explore FROMAGe's multimodal arithmetic capabilities both with textual and visual output.

## FROMAGe
For our research, we use the FROMAGe model (FROMAGe authors). Its vision-language architecture combines a frozen LLM with a frozen vision model. To map an encoder's output to the decoder of the complementary modality, a linear layer is trained. While this suffices to enable the language model to understand visual input (using the visual encoder and the linear translation layer), an additional step is needed for the opposite direction: To retrieve images from a database, a new [RET] token is introduced. The authors claim their method to be efficient as only the linear layer and the embedding for the [RET] token need to be updated, while keeping both the language and the vision model frozen. The model is trained both on an image captioning as well as an image-text retrieval task.\
(The authors):
- find strong few-shot abilities from training on image-caption pairs (while other models require web-scale interleaved image-text data)
- report better results for text-to-image retrieval for long and complex free-form text (compared to priorly existing models)
- show that existing capabilities of LLMs can be leveraged to visually grounded tasks (in particular, they demonstrate contextual image retrieval given sequences of interleaved images and text, strong zero-shot performance on visual dialogue, and improved sensitivity to discourse context for image retrieval)

## Related work
(Mikolov 2013) showed that the semantic similarity can be expressed as an arithmetic operation with the words' vector representations, i.e., their embeddings:
> "What is the word that is similar to *small* in the same sense as *biggest* is similar to *big*?"\
> X = vector("biggest") - vector("big") + vector("small")\
> the resulting word is the one whose embedding is closest to X, measured by cosine similarity\

(Tewel 2022) extend this arithmetic task to the multimodal vision-language field. They use CLIP's vision and language encoder to encode visual and textual input, perform the arithmetic operation in the latent embedding space, and use the obtained result to generate a text sequence. Their approach discovers detailed knowledge of the external world by moving in conceptual directions.
> "Who is the president of Germany?"\
> X = image(Obama) - image(USA) + image(Germany)\
> the resulting output is generated by the decoder when conditioned on X instead of an encoder's output\

(Koh 2023) perform image retrieval conditioned on multimodal input.
> image(cat) + "vector icon" returns a vector icon of a cat\

To our knowledge, no research has been published which investigates real-world knowledge using vision-laacnguage arithmetics with images as output.
> "Who is the president of Germany?"\
> X = image(Obama) - image(USA) + image(Germany)\
> based on X, an image is retrieved\

The FROMAGe authors build their work on recent advances in the field of LLMs, on research in the rather new field of vision-language models based on LLMs, as well as on approaches for parameter and resource efficient adaptation of pretrained models.


## Exposition

The paper delineates several key characteristics of the proposed model. First, it effectively leverages the capabilities of existing language models, including in-context learning and free-form text generation. Which allows the model to capitalize on the comprehensive knowledge encapsulated in these language models, thereby enhancing both the efficacy and efficiency of the system. Second, the model exhibits adaptness in managing cross-modal interactions, facilitating the processing of arbitrarily interleaved image and text inputs to generate coherent free-form text integrated with retrieved images. This versatility endows the model with indispensable multimodal dialogue capabilities, suitable for real-world scenarios requiring diverse input types. Finally, the model demonstrates robust zero-shot performance on grounded tasks like contextual image retrieval and multimodal dialogue, indicating a potent ability to generalize from learned concepts to new tasks without explicit task-specific training.

Such attributes inspire intriguing inquiries about the model's performance in the visual domain. Specifically, can the model effectively execute zero-shot visual arithmetic? Does the inherent in-context learning capability of large language models facilitate this task? How does the performance alter when engaged in multimodal arithmetic operations? These questions gain prominence given its design, which retrieves rather than generates images from the Conceptual Captions dataset. This makes it vulnerable to biases in the training and retrieval datasets which could potentially impact performance and results.

Our novel contributions include:

- Building upon the foundational paper that introduced FROMAGe, by offering novel insights into its visual arithmetic capabilities. Our findings will illustrate whether the model can successfully execute complex visual arithmetic operations, thereby broadening our comprehension of FROMAGe's functionality and potential applications.

- Evaluating the impact of latent few-shot in-context learning abilities of large language models (LLMs) on visual arithmetic. By investigating Chain-of-Thought reasoning on a task and modality that these LLMs are not trained on, we present the in-context abilities from a unique viewpoint divergent from previous literature. Our research discloses how the model effectively generalizes from limited examples, markedly enhancing the efficiency and precision of visual arithmetic operations.

- Demonstarting the influence of multimodal inputs on visual arithmetic. We furnish a deeper understanding of the interaction between different modalities in multimodal models, especially in tasks they are not trained on. The insights derived from this exploration bear significant implications for how multimodal models should be trained and utilized.

# Results



# Conclude

[comment]: <> (Technical limitations specific to FROMAGe:)
[comment]: <> (- does not always generate \[RET\] during inference)
[comment]: <> (- strong bias to produce regular text tokens)
[comment]: <> (Which are likely dou to its comprehensive pre-training regime (on text-only data).)
[comment]: <> (Somewhat alleviated by specifically prompting the model to ask it to show images.)

## Contributions



# References
