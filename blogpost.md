# Introduction

Breakthroughs in language modelling and computer vision have paved the way for remarkable advancements in the field of artificial intelligence (AI) in the last decade. These developments have been combined in recent years to propose models which operate across the modality boundary. A special interest has been paid to vision-language models, for which multiple architectures and model combinations have been proposed. A promising and efficient way to work across vision and language seems to be to map a frozen vision model to a frozen large language model (LMM), by training a linear translation layer.

The vision-language domain consists of many tasks, one of the most known and best researched ones probably being Image Captioning. Another, less investigated yet very intriguing task is visual arithmetics. Probably inspired by the arithmetics between word embeddings proposed by word2vec (/cite/) (e.g., "king" - "man" + "woman" = "queen"), it enables to express questions about real-world knowledge as an arithmetic operation across vision and language.

> "Who is the president of Germany?"\
> X = image(Obama) - image(USA) + image(Germany) &rarr; "Angela Merkel"

## Related work

In **ZeroCap**, /Tewel 2022/ describe their success in applying arithmetic operations to the visual embeddings "to generate knowledge of the external world". Furthermore, they extend their approach beyond the visual domain and manage to do calculations over a combination of visual and textual inputs by performing these operations in the latent embedding space. However, as they use CLIP (/cite/), a model that can handle visual and textual inputs but only generates textual outputs, their arithmetics are limited to output in natural language:

> "Who is the president of Germany?"\
> X = image(Obama) - image(USA) + image(Germany)\
> The resulting output is generated by the decoder when conditioned on X instead of an encoder's output.

Given the recent advances in the rather new field of vision-language models and approaches for parameter and resource efficient adaptation of pretrained models, (/Koh et al/) train **FROMAGe**, which is based on a frozen LLM and a frozen visual encoder. To ground the LLM to understand visual input, they train a linear translation layer between the visual encoder's output layer and the LLM's input layer. With this method, the visual embedding can be projected to the LLM's embedding space, enabling the LLM to handle it the same way as any textual input. The linear translation layer is trained on the image captioning task. In contrast to this image-to-text direction, they also propose a method to allow for a text-to-image direction. A new [RET] token (whose embeddings is learned during training) is added to the LLM's vocabulary and appended to the input captions during training. At inference time, the model therefore is able to generate the [RET] token, enabling seamlessly interleaving image retrieval within generated text.

In the FROMAGe paper, /Koh et al 2023/ come close to the field of multimodal arithmetics by performing image retrieval on multimodal input:

> image(cat) + "vector icon" returns a vector icon of a cat

To our knowledge, no research has been published which investigates real-world knowledge using vision-language arithmetics with images as output.

> "Who is the president of Germany?"\
> X = image(Obama) - image(USA) + image(Germany)\
> Based on X, an image is retrieved.

Recent research on the workings of LLMs by (/Wei et al. 2023/) has shown that using **chain-of-thought** (CoT) reasoning with these models can enable them to tackle complex arithmetic, commonsense and symbolic reasoning tasks. They show that standard prompting only provides a lower bound on the capabilities of LLMs, and that CoT prompting can improve performance.

## Exposition

The paper delineates several key characteristics of the proposed model. First, it effectively leverages the capabilities of existing language models, including in-context learning and free-form text generation. Which allows the model to capitalize on the comprehensive knowledge encapsulated in these language models, thereby enhancing both the efficacy and efficiency of the system. Second, the model exhibits adaptness in managing cross-modal interactions, facilitating the processing of arbitrarily interleaved image and text inputs to generate coherent free-form text integrated with retrieved images. This versatility endows the model with indispensable multimodal dialogue capabilities, suitable for real-world scenarios requiring diverse input types. Finally, the model demonstrates robust zero-shot performance on grounded tasks like contextual image retrieval and multimodal dialogue, indicating a potent ability to generalize from learned concepts to new tasks without explicit task-specific training.

Such attributes inspire intriguing inquiries about the model's performance in the visual domain. Specifically, can the model effectively execute zero-shot visual arithmetic? Does the inherent in-context learning capability of large language models facilitate this task? How does the performance alter when engaged in multimodal arithmetic operations? These questions gain prominence given its design, which retrieves rather than generates images from the Conceptual Captions dataset. This makes it vulnerable to biases in the training and retrieval datasets which could potentially impact performance and results.

Our novel contributions include:

- Building upon the foundational paper that introduced FROMAGe, by offering novel insights into its visual arithmetic capabilities. Our findings will illustrate whether the model can successfully execute complex visual arithmetic operations, thereby broadening our comprehension of FROMAGe's functionality and potential applications.

- Evaluating the impact of latent few-shot in-context learning abilities of large language models (LLMs) on visual arithmetic. By investigating Chain-of-Thought reasoning on a task and modality that these LLMs are not trained on, we present the in-context abilities from a unique viewpoint divergent from previous literature. Our research discloses how the model effectively generalizes from limited examples, markedly enhancing the efficiency and precision of visual arithmetic operations.

- Demonstarting the influence of multimodal inputs on visual arithmetic. We furnish a deeper understanding of the interaction between different modalities in multimodal models, especially in tasks they are not trained on. The insights derived from this exploration bear significant implications for how multimodal models should be trained and utilized.

# Results



# Conclude

[comment]: <> (Technical limitations specific to FROMAGe:)
[comment]: <> (- does not always generate \[RET\] during inference)
[comment]: <> (- strong bias to produce regular text tokens)
[comment]: <> (Which are likely dou to its comprehensive pre-training regime (on text-only data).)
[comment]: <> (Somewhat alleviated by specifically prompting the model to ask it to show images.)

## Contributions



# References
